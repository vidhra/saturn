{
  "GenerativeService": {
    "methods": [
      {
        "type": "function",
        "function": {
          "name": "generate_content",
          "description": "Generates a model response given an input\n``GenerateContentRequest``. Refer to the `text generation\nguide <https://ai.google.dev/gemini-api/docs/text-generation>`__\nfor detailed usage information. Input capabilities differ\nbetween models, including tuned models. Refer to the `model\nguide <https://ai.google.dev/gemini-api/docs/models/gemini>`__\nand `tuning\nguide <https://ai.google.dev/gemini-api/docs/model-tuning>`__\nfor details.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1\n\n    async def sample_generate_content():\n        # Create a client\n        client = generativelanguage_",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The name of the ``Model`` to use for generating the completion.  Format: ``name=models/{model}``.  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "contents": {
                "type": "MutableSequence[google.ai.generativelanguage_v1.types.Content]",
                "description": "Required. The content of the current conversation with the model.  For single-turn queries, this is a single instance. For multi-turn queries like `chat <https://ai.google.dev/gemini-api/docs/text-generation#chat>`__, this is a repeated field that contains the conversation history and the latest request.  This corresponds to the ``contents`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1.types.GenerateContentResponse: Response from the model supporting multiple candidate responses.  Safety ratings and content filtering are reported for both prompt in GenerateContentResponse.prompt_feedback and for each candidate in finish_reason and in safety_ratings. The API: - Returns either all requested candidates or none of them - Returns no candidates at all only if there was something wrong with the prompt (check prompt_feedback) - Reports feedback on each candidate in finish_reason and safety_ratings."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "contents",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [],
          "response_types": [
            "google.ai.generativelanguage_v1.types.GenerateContentResponse"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "stream_generate_content",
          "description": "Generates a `streamed\nresponse <https://ai.google.dev/gemini-api/docs/text-generation?lang=python#generate-a-text-stream>`__\nfrom the model given an input ``GenerateContentRequest``.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1\n\n    async def sample_stream_generate_content():\n        # Create a client\n        client = generativelanguage_v1.GenerativeServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1.GenerateContentRequest(\n            model=\"model_value\",\n        )\n\n        # Make the request\n        stream = await client.st",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The name of the ``Model`` to use for generating the completion.  Format: ``name=models/{model}``.  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "contents": {
                "type": "MutableSequence[google.ai.generativelanguage_v1.types.Content]",
                "description": "Required. The content of the current conversation with the model.  For single-turn queries, this is a single instance. For multi-turn queries like `chat <https://ai.google.dev/gemini-api/docs/text-generation#chat>`__, this is a repeated field that contains the conversation history and the latest request.  This corresponds to the ``contents`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: AsyncIterable[google.ai.generativelanguage_v1.types.GenerateContentResponse]: Response from the model supporting multiple candidate responses.  Safety ratings and content filtering are reported for both prompt in GenerateContentResponse.prompt_feedback and for each candidate in finish_reason and in safety_ratings. The API: - Returns either all requested candidates or none of them - Returns no candidates at all only if there was something wrong with the prompt (check prompt_feedback) - Reports feedback on each candidate in finish_reason and safety_ratings."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "contents",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [],
          "response_types": []
        }
      },
      {
        "type": "function",
        "function": {
          "name": "embed_content",
          "description": "Generates a text embedding vector from the input ``Content``\nusing the specified `Gemini Embedding\nmodel <https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding>`__.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1\n\n    async def sample_embed_content():\n        # Create a client\n        client = generativelanguage_v1.GenerativeServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1.EmbedContentRequest(\n            model=\"model_value\",\n        )\n\n        # Make the request\n        response = await client.embed_content(requ",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The model's resource name. This serves as an ID for the Model to use.  This name should match a model name returned by the ``ListModels`` method.  Format: ``models/{model}``  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "content": {
                "type": "google.ai.generativelanguage_v1.types.Content",
                "description": "Required. The content to embed. Only the ``parts.text`` fields will be counted.  This corresponds to the ``content`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1.types.EmbedContentResponse: The response to an EmbedContentRequest."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "content",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [],
          "response_types": [
            "google.ai.generativelanguage_v1.types.EmbedContentResponse"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "batch_embed_contents",
          "description": "Generates multiple embedding vectors from the input ``Content``\nwhich consists of a batch of strings represented as\n``EmbedContentRequest`` objects.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1\n\n    async def sample_batch_embed_contents():\n        # Create a client\n        client = generativelanguage_v1.GenerativeServiceAsyncClient()\n\n        # Initialize request argument(s)\n        requests = generativelanguage_v1.EmbedContentRequest()\n        requests.model = \"model_value\"\n\n        request = generativelanguage_v1.BatchEmbedContentsRequest(\n            model=\"model_value\",\n ",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The model's resource name. This serves as an ID for the Model to use.  This name should match a model name returned by the ``ListModels`` method.  Format: ``models/{model}``  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "requests": {
                "type": "MutableSequence[google.ai.generativelanguage_v1.types.EmbedContentRequest]",
                "description": "Required. Embed requests for the batch. The model in each of these requests must match the model specified ``BatchEmbedContentsRequest.model``.  This corresponds to the ``requests`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1.types.BatchEmbedContentsResponse: The response to a BatchEmbedContentsRequest."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "requests",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [],
          "response_types": [
            "google.ai.generativelanguage_v1.types.BatchEmbedContentsResponse"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "count_tokens",
          "description": "Runs a model's tokenizer on input ``Content`` and returns the\ntoken count. Refer to the `tokens\nguide <https://ai.google.dev/gemini-api/docs/tokens>`__ to learn\nmore about tokens.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1\n\n    async def sample_count_tokens():\n        # Create a client\n        client = generativelanguage_v1.GenerativeServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1.CountTokensRequest(\n            model=\"model_value\",\n        )\n\n        # Make the request\n        response = await client.count_tokens(reque",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The model's resource name. This serves as an ID for the Model to use.  This name should match a model name returned by the ``ListModels`` method.  Format: ``models/{model}``  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "contents": {
                "type": "MutableSequence[google.ai.generativelanguage_v1.types.Content]",
                "description": "Optional. The input given to the model as a prompt. This field is ignored when ``generate_content_request`` is set.  This corresponds to the ``contents`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1.types.CountTokensResponse: A response from CountTokens.  It returns the model's token_count for the prompt."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "contents",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [],
          "response_types": [
            "google.ai.generativelanguage_v1.types.CountTokensResponse"
          ]
        }
      }
    ]
  },
  "ModelService": {
    "methods": [
      {
        "type": "function",
        "function": {
          "name": "get_model",
          "description": "Gets information about a specific ``Model`` such as its version\nnumber, token limits,\n`parameters <https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters>`__\nand other metadata. Refer to the `Gemini models\nguide <https://ai.google.dev/gemini-api/docs/models/gemini>`__\nfor detailed model information.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1\n\n    async def sample_get_model():\n        # Create a client\n        client = generativelanguage_v1.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1.Get",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "name": {
                "type": "str",
                "description": "Required. The resource name of the model.  This name should match a model name returned by the ``ListModels`` method.  Format: ``models/{model}``  This corresponds to the ``name`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1.types.Model: Information about a Generative Language Model."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "name",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [],
          "response_types": [
            "google.ai.generativelanguage_v1.types.Model"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "list_models",
          "description": "Lists the\n```Model``\\ s <https://ai.google.dev/gemini-api/docs/models/gemini>`__\navailable through the Gemini API.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1\n\n    async def sample_list_models():\n        # Create a client\n        client = generativelanguage_v1.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1.ListModelsRequest(\n        )\n\n        # Make the request\n        page_result = client.list_models(request=request)\n\n        # Handle the response\n        async for response in page_result:\n            print(respo",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "page_size": {
                "type": "int",
                "description": "The maximum number of ``Models`` to return (per page).  If unspecified, 50 models will be returned per page. This method returns at most 1000 models per page, even if you pass a larger page_size.  This corresponds to the ``page_size`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "page_token": {
                "type": "str",
                "description": "A page token, received from a previous ``ListModels`` call.  Provide the ``page_token`` returned by one request as an argument to the next request to retrieve the next page.  When paginating, all other parameters provided to ``ListModels`` must match the call that provided the page token.  This corresponds to the ``page_token`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1.services.model_service.pagers.ListModelsAsyncPager: Response from ListModel containing a paginated list of Models.  Iterating over this object will yield results and resolve additional pages automatically."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "page_size",
              "page_token",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [],
          "response_types": [
            "google.ai.generativelanguage_v1.services.model_service.pagers.ListModelsAsyncPager"
          ]
        }
      }
    ]
  }
}