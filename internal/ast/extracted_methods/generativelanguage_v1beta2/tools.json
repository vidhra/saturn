{
  "DiscussService": {
    "methods": [
      {
        "type": "function",
        "function": {
          "name": "generate_message",
          "description": "Generates a response from the model given an input\n``MessagePrompt``.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta2\n\n    async def sample_generate_message():\n        # Create a client\n        client = generativelanguage_v1beta2.DiscussServiceAsyncClient()\n\n        # Initialize request argument(s)\n        prompt = generativelanguage_v1beta2.MessagePrompt()\n        prompt.messages.content = \"content_value\"\n\n        request = generativelanguage_v1beta2.GenerateMessageRequest(\n            model=\"model_value\",\n            prompt=prompt,\n        )\n\n        # Make the request\n  ",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The name of the model to use.  Format: ``name=models/{model}``.  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "prompt": {
                "type": "google.ai.generativelanguage_v1beta2.types.MessagePrompt",
                "description": "Required. The structured textual input given to the model as a prompt. Given a prompt, the model will return what it predicts is the next message in the discussion.  This corresponds to the ``prompt`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "temperature": {
                "type": "float",
                "description": "Optional. Controls the randomness of the output.  Values can range over ``[0.0,1.0]``, inclusive. A value closer to ``1.0`` will produce responses that are more varied, while a value closer to ``0.0`` will typically result in less surprising responses from the model.  This corresponds to the ``temperature`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "candidate_count": {
                "type": "int",
                "description": "Optional. The number of generated response messages to return.  This value must be between ``[1, 8]``, inclusive. If unset, this will default to ``1``.  This corresponds to the ``candidate_count`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "top_p": {
                "type": "float",
                "description": "Optional. The maximum cumulative probability of tokens to consider when sampling.  The model uses combined Top-k and nucleus sampling.  Nucleus sampling considers the smallest set of tokens whose probability sum is at least ``top_p``.  This corresponds to the ``top_p`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "top_k": {
                "type": "int",
                "description": "Optional. The maximum number of tokens to consider when sampling.  The model uses combined Top-k and nucleus sampling.  Top-k sampling considers the set of ``top_k`` most probable tokens.  This corresponds to the ``top_k`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta2.types.GenerateMessageResponse: The response from the model.  This includes candidate messages and conversation history in the form of chronologically-ordered messages."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "prompt",
              "temperature",
              "candidate_count",
              "top_p",
              "top_k",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "discuss_service.GenerateMessageRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "count_message_tokens",
          "description": "Runs a model's tokenizer on a string and returns the\ntoken count.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta2\n\n    async def sample_count_message_tokens():\n        # Create a client\n        client = generativelanguage_v1beta2.DiscussServiceAsyncClient()\n\n        # Initialize request argument(s)\n        prompt = generativelanguage_v1beta2.MessagePrompt()\n        prompt.messages.content = \"content_value\"\n\n        request = generativelanguage_v1beta2.CountMessageTokensRequest(\n            model=\"model_value\",\n            prompt=prompt,\n        )\n\n        # Make the request",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The model's resource name. This serves as an ID for the Model to use.  This name should match a model name returned by the ``ListModels`` method.  Format: ``models/{model}``  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "prompt": {
                "type": "google.ai.generativelanguage_v1beta2.types.MessagePrompt",
                "description": "Required. The prompt, whose token count is to be returned.  This corresponds to the ``prompt`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta2.types.CountMessageTokensResponse: A response from CountMessageTokens.  It returns the model's token_count for the prompt."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "prompt",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "discuss_service.CountMessageTokensRequest"
          ]
        }
      }
    ]
  },
  "ModelService": {
    "methods": [
      {
        "type": "function",
        "function": {
          "name": "get_model",
          "description": "Gets information about a specific Model.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta2\n\n    async def sample_get_model():\n        # Create a client\n        client = generativelanguage_v1beta2.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta2.GetModelRequest(\n            name=\"name_value\",\n        )\n\n        # Make the request\n        response = await client.get_model(request=request)\n\n        # Handle the response\n        print(response)\n\nArgs:\n    request (Optional[Union[google.ai.generativelanguage_v1beta2.t",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "name": {
                "type": "str",
                "description": "Required. The resource name of the model.  This name should match a model name returned by the ``ListModels`` method.  Format: ``models/{model}``  This corresponds to the ``name`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta2.types.Model: Information about a Generative Language Model."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "name",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "model_service.GetModelRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "list_models",
          "description": "Lists models available through the API.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta2\n\n    async def sample_list_models():\n        # Create a client\n        client = generativelanguage_v1beta2.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta2.ListModelsRequest(\n        )\n\n        # Make the request\n        page_result = client.list_models(request=request)\n\n        # Handle the response\n        async for response in page_result:\n            print(response)\n\nArgs:\n    request (Optional[Union[google.ai.generative",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "page_size": {
                "type": "int",
                "description": "The maximum number of ``Models`` to return (per page).  The service may return fewer models. If unspecified, at most 50 models will be returned per page. This method returns at most 1000 models per page, even if you pass a larger page_size.  This corresponds to the ``page_size`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "page_token": {
                "type": "str",
                "description": "A page token, received from a previous ``ListModels`` call.  Provide the ``page_token`` returned by one request as an argument to the next request to retrieve the next page.  When paginating, all other parameters provided to ``ListModels`` must match the call that provided the page token.  This corresponds to the ``page_token`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta2.services.model_service.pagers.ListModelsAsyncPager: Response from ListModel containing a paginated list of Models.  Iterating over this object will yield results and resolve additional pages automatically."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "page_size",
              "page_token",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "model_service.ListModelsRequest"
          ]
        }
      }
    ]
  },
  "TextService": {
    "methods": [
      {
        "type": "function",
        "function": {
          "name": "generate_text",
          "description": "Generates a response from the model given an input\nmessage.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta2\n\n    async def sample_generate_text():\n        # Create a client\n        client = generativelanguage_v1beta2.TextServiceAsyncClient()\n\n        # Initialize request argument(s)\n        prompt = generativelanguage_v1beta2.TextPrompt()\n        prompt.text = \"text_value\"\n\n        request = generativelanguage_v1beta2.GenerateTextRequest(\n            model=\"model_value\",\n            prompt=prompt,\n        )\n\n        # Make the request\n        response = await client.generat",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The model name to use with the format name=models/{model}.  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "prompt": {
                "type": "google.ai.generativelanguage_v1beta2.types.TextPrompt",
                "description": "Required. The free-form input text given to the model as a prompt. Given a prompt, the model will generate a TextCompletion response it predicts as the completion of the input text.  This corresponds to the ``prompt`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "temperature": {
                "type": "float",
                "description": "Controls the randomness of the output. Note: The default value varies by model, see the ``Model.temperature`` attribute of the ``Model`` returned the ``getModel`` function.  Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied and creative, while a value closer to 0.0 will typically result in more straightforward responses from the model.  This corresponds to the ``temperature`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "candidate_count": {
                "type": "int",
                "description": "Number of generated responses to return.  This value must be between [1, 8], inclusive. If unset, this will default to 1.  This corresponds to the ``candidate_count`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "max_output_tokens": {
                "type": "int",
                "description": "The maximum number of tokens to include in a candidate. If unset, this will default to 64.  This corresponds to the ``max_output_tokens`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "top_p": {
                "type": "float",
                "description": "The maximum cumulative probability of tokens to consider when sampling.  The model uses combined Top-k and nucleus sampling.  Tokens are sorted based on their assigned probabilities so that only the most liekly tokens are considered. Top-k sampling directly limits the maximum number of tokens to consider, while Nucleus sampling limits number of tokens based on the cumulative probability.  Note: The default value varies by model, see the ``Model.top_p`` attribute of the ``Model`` returned the ``getModel`` function.  This corresponds to the ``top_p`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "top_k": {
                "type": "int",
                "description": "The maximum number of tokens to consider when sampling.  The model uses combined Top-k and nucleus sampling.  Top-k sampling considers the set of ``top_k`` most probable tokens. Defaults to 40.  Note: The default value varies by model, see the ``Model.top_k`` attribute of the ``Model`` returned the ``getModel`` function.  This corresponds to the ``top_k`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta2.types.GenerateTextResponse: The response from the model, including candidate completions."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "prompt",
              "temperature",
              "candidate_count",
              "max_output_tokens",
              "top_p",
              "top_k",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "text_service.GenerateTextRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "embed_text",
          "description": "Generates an embedding from the model given an input\nmessage.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta2\n\n    async def sample_embed_text():\n        # Create a client\n        client = generativelanguage_v1beta2.TextServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta2.EmbedTextRequest(\n            model=\"model_value\",\n            text=\"text_value\",\n        )\n\n        # Make the request\n        response = await client.embed_text(request=request)\n\n        # Handle the response\n        print(response)\n\nArgs:\n    reques",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The model name to use with the format model=models/{model}.  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "text": {
                "type": "str",
                "description": "Required. The free-form input text that the model will turn into an embedding.  This corresponds to the ``text`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta2.types.EmbedTextResponse: The response to a EmbedTextRequest."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "text",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "text_service.EmbedTextRequest"
          ]
        }
      }
    ]
  }
}