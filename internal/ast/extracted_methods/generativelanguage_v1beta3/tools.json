{
  "DiscussService": {
    "methods": [
      {
        "type": "function",
        "function": {
          "name": "generate_message",
          "description": "Generates a response from the model given an input\n``MessagePrompt``.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_generate_message():\n        # Create a client\n        client = generativelanguage_v1beta3.DiscussServiceAsyncClient()\n\n        # Initialize request argument(s)\n        prompt = generativelanguage_v1beta3.MessagePrompt()\n        prompt.messages.content = \"content_value\"\n\n        request = generativelanguage_v1beta3.GenerateMessageRequest(\n            model=\"model_value\",\n            prompt=prompt,\n        )\n\n        # Make the request\n  ",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The name of the model to use.  Format: ``name=models/{model}``.  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "prompt": {
                "type": "google.ai.generativelanguage_v1beta3.types.MessagePrompt",
                "description": "Required. The structured textual input given to the model as a prompt. Given a prompt, the model will return what it predicts is the next message in the discussion.  This corresponds to the ``prompt`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "temperature": {
                "type": "float",
                "description": "Optional. Controls the randomness of the output.  Values can range over ``[0.0,1.0]``, inclusive. A value closer to ``1.0`` will produce responses that are more varied, while a value closer to ``0.0`` will typically result in less surprising responses from the model.  This corresponds to the ``temperature`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "candidate_count": {
                "type": "int",
                "description": "Optional. The number of generated response messages to return.  This value must be between ``[1, 8]``, inclusive. If unset, this will default to ``1``.  This corresponds to the ``candidate_count`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "top_p": {
                "type": "float",
                "description": "Optional. The maximum cumulative probability of tokens to consider when sampling.  The model uses combined Top-k and nucleus sampling.  Nucleus sampling considers the smallest set of tokens whose probability sum is at least ``top_p``.  This corresponds to the ``top_p`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "top_k": {
                "type": "int",
                "description": "Optional. The maximum number of tokens to consider when sampling.  The model uses combined Top-k and nucleus sampling.  Top-k sampling considers the set of ``top_k`` most probable tokens.  This corresponds to the ``top_k`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.GenerateMessageResponse: The response from the model.  This includes candidate messages and conversation history in the form of chronologically-ordered messages."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "prompt",
              "temperature",
              "candidate_count",
              "top_p",
              "top_k",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "discuss_service.GenerateMessageRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "count_message_tokens",
          "description": "Runs a model's tokenizer on a string and returns the\ntoken count.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_count_message_tokens():\n        # Create a client\n        client = generativelanguage_v1beta3.DiscussServiceAsyncClient()\n\n        # Initialize request argument(s)\n        prompt = generativelanguage_v1beta3.MessagePrompt()\n        prompt.messages.content = \"content_value\"\n\n        request = generativelanguage_v1beta3.CountMessageTokensRequest(\n            model=\"model_value\",\n            prompt=prompt,\n        )\n\n        # Make the request",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The model's resource name. This serves as an ID for the Model to use.  This name should match a model name returned by the ``ListModels`` method.  Format: ``models/{model}``  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "prompt": {
                "type": "google.ai.generativelanguage_v1beta3.types.MessagePrompt",
                "description": "Required. The prompt, whose token count is to be returned.  This corresponds to the ``prompt`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.CountMessageTokensResponse: A response from CountMessageTokens.  It returns the model's token_count for the prompt."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "prompt",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "discuss_service.CountMessageTokensRequest"
          ]
        }
      }
    ]
  },
  "ModelService": {
    "methods": [
      {
        "type": "function",
        "function": {
          "name": "get_model",
          "description": "Gets information about a specific Model.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_get_model():\n        # Create a client\n        client = generativelanguage_v1beta3.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.GetModelRequest(\n            name=\"name_value\",\n        )\n\n        # Make the request\n        response = await client.get_model(request=request)\n\n        # Handle the response\n        print(response)\n\nArgs:\n    request (Optional[Union[google.ai.generativelanguage_v1beta3.t",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "name": {
                "type": "str",
                "description": "Required. The resource name of the model.  This name should match a model name returned by the ``ListModels`` method.  Format: ``models/{model}``  This corresponds to the ``name`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.Model: Information about a Generative Language Model."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "name",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "model_service.GetModelRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "list_models",
          "description": "Lists models available through the API.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_list_models():\n        # Create a client\n        client = generativelanguage_v1beta3.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.ListModelsRequest(\n        )\n\n        # Make the request\n        page_result = client.list_models(request=request)\n\n        # Handle the response\n        async for response in page_result:\n            print(response)\n\nArgs:\n    request (Optional[Union[google.ai.generative",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "page_size": {
                "type": "int",
                "description": "The maximum number of ``Models`` to return (per page).  The service may return fewer models. If unspecified, at most 50 models will be returned per page. This method returns at most 1000 models per page, even if you pass a larger page_size.  This corresponds to the ``page_size`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "page_token": {
                "type": "str",
                "description": "A page token, received from a previous ``ListModels`` call.  Provide the ``page_token`` returned by one request as an argument to the next request to retrieve the next page.  When paginating, all other parameters provided to ``ListModels`` must match the call that provided the page token.  This corresponds to the ``page_token`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.services.model_service.pagers.ListModelsAsyncPager: Response from ListModel containing a paginated list of Models.  Iterating over this object will yield results and resolve additional pages automatically."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "page_size",
              "page_token",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "model_service.ListModelsRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "get_tuned_model",
          "description": "Gets information about a specific TunedModel.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_get_tuned_model():\n        # Create a client\n        client = generativelanguage_v1beta3.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.GetTunedModelRequest(\n            name=\"name_value\",\n        )\n\n        # Make the request\n        response = await client.get_tuned_model(request=request)\n\n        # Handle the response\n        print(response)\n\nArgs:\n    request (Optional[Union[google.ai.genera",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "name": {
                "type": "str",
                "description": "Required. The resource name of the model.  Format: ``tunedModels/my-model-id``  This corresponds to the ``name`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.TunedModel: A fine-tuned model created using ModelService.CreateTunedModel."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "name",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "model_service.GetTunedModelRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "list_tuned_models",
          "description": "Lists tuned models owned by the user.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_list_tuned_models():\n        # Create a client\n        client = generativelanguage_v1beta3.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.ListTunedModelsRequest(\n        )\n\n        # Make the request\n        page_result = client.list_tuned_models(request=request)\n\n        # Handle the response\n        async for response in page_result:\n            print(response)\n\nArgs:\n    request (Optional[Union[googl",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "page_size": {
                "type": "int",
                "description": "Optional. The maximum number of ``TunedModels`` to return (per page). The service may return fewer tuned models.  If unspecified, at most 10 tuned models will be returned. This method returns at most 1000 models per page, even if you pass a larger page_size.  This corresponds to the ``page_size`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "page_token": {
                "type": "str",
                "description": "Optional. A page token, received from a previous ``ListTunedModels`` call.  Provide the ``page_token`` returned by one request as an argument to the next request to retrieve the next page.  When paginating, all other parameters provided to ``ListTunedModels`` must match the call that provided the page token.  This corresponds to the ``page_token`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.services.model_service.pagers.ListTunedModelsAsyncPager: Response from ListTunedModels containing a paginated list of Models.  Iterating over this object will yield results and resolve additional pages automatically."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "page_size",
              "page_token",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "model_service.ListTunedModelsRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "create_tuned_model",
          "description": "Creates a tuned model. Intermediate tuning progress (if any) is\naccessed through the [google.longrunning.Operations] service.\n\nStatus and results can be accessed through the Operations\nservice. Example: GET\n/v1/tunedModels/az2mb0bpw6i/operations/000-111-222\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_create_tuned_model():\n        # Create a client\n        client = generativelanguage_v1beta3.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        tuned_model = generativelanguage_v1beta3.TunedModel()\n        tuned_model.tuning_tas",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "tuned_model": {
                "type": "google.ai.generativelanguage_v1beta3.types.TunedModel",
                "description": "Required. The tuned model to create. This corresponds to the ``tuned_model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "tuned_model_id": {
                "type": "str",
                "description": "Optional. The unique id for the tuned model if specified. This value should be up to 40 characters, the first character must be a letter, the last could be a letter or a number. The id must match the regular expression: `a-z <[a-z0-9-]{0,38}[a-z0-9]>`__?.  This corresponds to the ``tuned_model_id`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.api_core.operation_async.AsyncOperation: An object representing a long-running operation.  The result type for the operation will be :class:`google.ai.generativelanguage_v1beta3.types.TunedModel` A fine-tuned model created using ModelService.CreateTunedModel."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "tuned_model",
              "tuned_model_id",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "model_service.CreateTunedModelRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "update_tuned_model",
          "description": "Updates a tuned model.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_update_tuned_model():\n        # Create a client\n        client = generativelanguage_v1beta3.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        tuned_model = generativelanguage_v1beta3.TunedModel()\n        tuned_model.tuning_task.training_data.examples.examples.text_input = \"text_input_value\"\n        tuned_model.tuning_task.training_data.examples.examples.output = \"output_value\"\n\n        request = generativelanguage_v1beta3.UpdateTunedModelRequest(\n          ",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "tuned_model": {
                "type": "google.ai.generativelanguage_v1beta3.types.TunedModel",
                "description": "Required. The tuned model to update. This corresponds to the ``tuned_model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "update_mask": {
                "type": "google.protobuf.field_mask_pb2.FieldMask",
                "description": "Required. The list of fields to update.  This corresponds to the ``update_mask`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.TunedModel: A fine-tuned model created using ModelService.CreateTunedModel."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "tuned_model",
              "update_mask",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "model_service.UpdateTunedModelRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "delete_tuned_model",
          "description": "Deletes a tuned model.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_delete_tuned_model():\n        # Create a client\n        client = generativelanguage_v1beta3.ModelServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.DeleteTunedModelRequest(\n            name=\"name_value\",\n        )\n\n        # Make the request\n        await client.delete_tuned_model(request=request)\n\nArgs:\n    request (Optional[Union[google.ai.generativelanguage_v1beta3.types.DeleteTunedModelRequest, dict]]):\n        The request",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "name": {
                "type": "str",
                "description": "Required. The resource name of the model. Format: ``tunedModels/my-model-id``  This corresponds to the ``name`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "name",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "model_service.DeleteTunedModelRequest"
          ]
        }
      }
    ]
  },
  "PermissionService": {
    "methods": [
      {
        "type": "function",
        "function": {
          "name": "create_permission",
          "description": "Create a permission to a specific resource.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_create_permission():\n        # Create a client\n        client = generativelanguage_v1beta3.PermissionServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.CreatePermissionRequest(\n            parent=\"parent_value\",\n        )\n\n        # Make the request\n        response = await client.create_permission(request=request)\n\n        # Handle the response\n        print(response)\n\nArgs:\n    request (Optional[Union[go",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "parent": {
                "type": "str",
                "description": "Required. The parent resource of the ``Permission``. Format: tunedModels/{tuned_model}  This corresponds to the ``parent`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "permission": {
                "type": "google.ai.generativelanguage_v1beta3.types.Permission",
                "description": "Required. The permission to create. This corresponds to the ``permission`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.Permission: Permission resource grants user, group or the rest of the world access to the PaLM API resource (e.g. a tuned model, file).  A role is a collection of permitted operations that allows users to perform specific actions on PaLM API resources. To make them available to users, groups, or service accounts, you assign roles. When you assign a role, you grant permissions that the role contains.  There are three concentric roles. Each role is a superset of the previous role's permitted operations:  - reader can use the resource (e.g. tuned model) for inference - writer has reader's permissions and additionally can edit and share - owner has writer's permissions and additionally can delete"
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "parent",
              "permission",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "permission_service.CreatePermissionRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "get_permission",
          "description": "Gets information about a specific Permission.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_get_permission():\n        # Create a client\n        client = generativelanguage_v1beta3.PermissionServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.GetPermissionRequest(\n            name=\"name_value\",\n        )\n\n        # Make the request\n        response = await client.get_permission(request=request)\n\n        # Handle the response\n        print(response)\n\nArgs:\n    request (Optional[Union[google.ai.gen",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "name": {
                "type": "str",
                "description": "Required. The resource name of the permission.  Format: ``tunedModels/{tuned_model}permissions/{permission}``  This corresponds to the ``name`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.Permission: Permission resource grants user, group or the rest of the world access to the PaLM API resource (e.g. a tuned model, file).  A role is a collection of permitted operations that allows users to perform specific actions on PaLM API resources. To make them available to users, groups, or service accounts, you assign roles. When you assign a role, you grant permissions that the role contains.  There are three concentric roles. Each role is a superset of the previous role's permitted operations:  - reader can use the resource (e.g. tuned model) for inference - writer has reader's permissions and additionally can edit and share - owner has writer's permissions and additionally can delete"
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "name",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "permission_service.GetPermissionRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "list_permissions",
          "description": "Lists permissions for the specific resource.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_list_permissions():\n        # Create a client\n        client = generativelanguage_v1beta3.PermissionServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.ListPermissionsRequest(\n            parent=\"parent_value\",\n        )\n\n        # Make the request\n        page_result = client.list_permissions(request=request)\n\n        # Handle the response\n        async for response in page_result:\n            print(respo",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "parent": {
                "type": "str",
                "description": "Required. The parent resource of the permissions. Format: tunedModels/{tuned_model}  This corresponds to the ``parent`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.services.permission_service.pagers.ListPermissionsAsyncPager: Response from ListPermissions containing a paginated list of permissions.  Iterating over this object will yield results and resolve additional pages automatically."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "parent",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "permission_service.ListPermissionsRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "update_permission",
          "description": "Updates the permission.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_update_permission():\n        # Create a client\n        client = generativelanguage_v1beta3.PermissionServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.UpdatePermissionRequest(\n        )\n\n        # Make the request\n        response = await client.update_permission(request=request)\n\n        # Handle the response\n        print(response)\n\nArgs:\n    request (Optional[Union[google.ai.generativelanguage_v1beta3.types.UpdatePermissi",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "permission": {
                "type": "google.ai.generativelanguage_v1beta3.types.Permission",
                "description": "Required. The permission to update.  The permission's ``name`` field is used to identify the permission to update.  This corresponds to the ``permission`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "update_mask": {
                "type": "google.protobuf.field_mask_pb2.FieldMask",
                "description": "Required. The list of fields to update. Accepted ones:  -  role (``Permission.role`` field)  This corresponds to the ``update_mask`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.Permission: Permission resource grants user, group or the rest of the world access to the PaLM API resource (e.g. a tuned model, file).  A role is a collection of permitted operations that allows users to perform specific actions on PaLM API resources. To make them available to users, groups, or service accounts, you assign roles. When you assign a role, you grant permissions that the role contains.  There are three concentric roles. Each role is a superset of the previous role's permitted operations:  - reader can use the resource (e.g. tuned model) for inference - writer has reader's permissions and additionally can edit and share - owner has writer's permissions and additionally can delete"
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "permission",
              "update_mask",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "permission_service.UpdatePermissionRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "delete_permission",
          "description": "Deletes the permission.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_delete_permission():\n        # Create a client\n        client = generativelanguage_v1beta3.PermissionServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.DeletePermissionRequest(\n            name=\"name_value\",\n        )\n\n        # Make the request\n        await client.delete_permission(request=request)\n\nArgs:\n    request (Optional[Union[google.ai.generativelanguage_v1beta3.types.DeletePermissionRequest, dict]]):\n        The req",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "name": {
                "type": "str",
                "description": "Required. The resource name of the permission. Format: ``tunedModels/{tuned_model}/permissions/{permission}``  This corresponds to the ``name`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "name",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "permission_service.DeletePermissionRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "transfer_ownership",
          "description": "Transfers ownership of the tuned model.\nThis is the only way to change ownership of the tuned\nmodel. The current owner will be downgraded to writer\nrole.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_transfer_ownership():\n        # Create a client\n        client = generativelanguage_v1beta3.PermissionServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.TransferOwnershipRequest(\n            name=\"name_value\",\n            email_address=\"email_address_value\",\n        )\n\n        # Make the request\n  ",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "permission_service.TransferOwnershipRequest"
          ]
        }
      }
    ]
  },
  "TextService": {
    "methods": [
      {
        "type": "function",
        "function": {
          "name": "generate_text",
          "description": "Generates a response from the model given an input\nmessage.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_generate_text():\n        # Create a client\n        client = generativelanguage_v1beta3.TextServiceAsyncClient()\n\n        # Initialize request argument(s)\n        prompt = generativelanguage_v1beta3.TextPrompt()\n        prompt.text = \"text_value\"\n\n        request = generativelanguage_v1beta3.GenerateTextRequest(\n            model=\"model_value\",\n            prompt=prompt,\n        )\n\n        # Make the request\n        response = await client.generat",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The name of the ``Model`` or ``TunedModel`` to use for generating the completion. Examples: models/text-bison-001 tunedModels/sentence-translator-u3b7m  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "prompt": {
                "type": "google.ai.generativelanguage_v1beta3.types.TextPrompt",
                "description": "Required. The free-form input text given to the model as a prompt. Given a prompt, the model will generate a TextCompletion response it predicts as the completion of the input text.  This corresponds to the ``prompt`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "temperature": {
                "type": "float",
                "description": "Optional. Controls the randomness of the output. Note: The default value varies by model, see the ``Model.temperature`` attribute of the ``Model`` returned the ``getModel`` function.  Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied and creative, while a value closer to 0.0 will typically result in more straightforward responses from the model.  This corresponds to the ``temperature`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "candidate_count": {
                "type": "int",
                "description": "Optional. Number of generated responses to return.  This value must be between [1, 8], inclusive. If unset, this will default to 1.  This corresponds to the ``candidate_count`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "max_output_tokens": {
                "type": "int",
                "description": "Optional. The maximum number of tokens to include in a candidate.  If unset, this will default to output_token_limit specified in the ``Model`` specification.  This corresponds to the ``max_output_tokens`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "top_p": {
                "type": "float",
                "description": "Optional. The maximum cumulative probability of tokens to consider when sampling.  The model uses combined Top-k and nucleus sampling.  Tokens are sorted based on their assigned probabilities so that only the most likely tokens are considered. Top-k sampling directly limits the maximum number of tokens to consider, while Nucleus sampling limits number of tokens based on the cumulative probability.  Note: The default value varies by model, see the ``Model.top_p`` attribute of the ``Model`` returned the ``getModel`` function.  This corresponds to the ``top_p`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "top_k": {
                "type": "int",
                "description": "Optional. The maximum number of tokens to consider when sampling.  The model uses combined Top-k and nucleus sampling.  Top-k sampling considers the set of ``top_k`` most probable tokens. Defaults to 40.  Note: The default value varies by model, see the ``Model.top_k`` attribute of the ``Model`` returned the ``getModel`` function.  This corresponds to the ``top_k`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.GenerateTextResponse: The response from the model, including candidate completions."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "prompt",
              "temperature",
              "candidate_count",
              "max_output_tokens",
              "top_p",
              "top_k",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "text_service.GenerateTextRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "embed_text",
          "description": "Generates an embedding from the model given an input\nmessage.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_embed_text():\n        # Create a client\n        client = generativelanguage_v1beta3.TextServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.EmbedTextRequest(\n            model=\"model_value\",\n            text=\"text_value\",\n        )\n\n        # Make the request\n        response = await client.embed_text(request=request)\n\n        # Handle the response\n        print(response)\n\nArgs:\n    reques",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The model name to use with the format model=models/{model}.  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "text": {
                "type": "str",
                "description": "Required. The free-form input text that the model will turn into an embedding.  This corresponds to the ``text`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.EmbedTextResponse: The response to a EmbedTextRequest."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "text",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "text_service.EmbedTextRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "batch_embed_text",
          "description": "Generates multiple embeddings from the model given\ninput text in a synchronous call.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_batch_embed_text():\n        # Create a client\n        client = generativelanguage_v1beta3.TextServiceAsyncClient()\n\n        # Initialize request argument(s)\n        request = generativelanguage_v1beta3.BatchEmbedTextRequest(\n            model=\"model_value\",\n            texts=['texts_value1', 'texts_value2'],\n        )\n\n        # Make the request\n        response = await client.batch_embed_text(request=request)\n\n        # ",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The name of the ``Model`` to use for generating the embedding. Examples: models/embedding-gecko-001  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "texts": {
                "type": "MutableSequence[str]",
                "description": "Required. The free-form input texts that the model will turn into an embedding.  The current limit is 100 texts, over which an error will be thrown.  This corresponds to the ``texts`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.BatchEmbedTextResponse: The response to a EmbedTextRequest."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "texts",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "text_service.BatchEmbedTextRequest"
          ]
        }
      },
      {
        "type": "function",
        "function": {
          "name": "count_text_tokens",
          "description": "Runs a model's tokenizer on a text and returns the\ntoken count.\n\n.. code-block:: python\n\n    # This snippet has been automatically generated and should be regarded as a\n    # code template only.\n    # It will require modifications to work:\n    # - It may require correct/in-range values for request initialization.\n    # - It may require specifying regional endpoints when creating the service\n    #   client as shown in:\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\n    from google.ai import generativelanguage_v1beta3\n\n    async def sample_count_text_tokens():\n        # Create a client\n        client = generativelanguage_v1beta3.TextServiceAsyncClient()\n\n        # Initialize request argument(s)\n        prompt = generativelanguage_v1beta3.TextPrompt()\n        prompt.text = \"text_value\"\n\n        request = generativelanguage_v1beta3.CountTextTokensRequest(\n            model=\"model_value\",\n            prompt=prompt,\n        )\n\n        # Make the request\n        response = await cli",
          "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "string",
                "description": ""
              },
              "model": {
                "type": "str",
                "description": "Required. The model's resource name. This serves as an ID for the Model to use.  This name should match a model name returned by the ``ListModels`` method.  Format: ``models/{model}``  This corresponds to the ``model`` field on the ``request`` instance; if ``request`` is provided, this should not be set."
              },
              "prompt": {
                "type": "google.ai.generativelanguage_v1beta3.types.TextPrompt",
                "description": "Required. The free-form input text given to the model as a prompt.  This corresponds to the ``prompt`` field on the ``request`` instance; if ``request`` is provided, this should not be set. retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any, should be retried. timeout (float): The timeout for this request. metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.  Returns: google.ai.generativelanguage_v1beta3.types.CountTextTokensResponse: A response from CountTextTokens.  It returns the model's token_count for the prompt."
              },
              "retry": {
                "type": "string",
                "description": ""
              },
              "timeout": {
                "type": "string",
                "description": ""
              },
              "metadata": {
                "type": "string",
                "description": ""
              }
            },
            "required": [
              "request",
              "model",
              "prompt",
              "retry",
              "timeout",
              "metadata"
            ]
          },
          "request_types": [
            "text_service.CountTextTokensRequest"
          ]
        }
      }
    ]
  }
}