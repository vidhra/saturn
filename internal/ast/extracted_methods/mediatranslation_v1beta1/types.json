{
  "/Users/prashanthvarma/Saturn/internal/ast/google-cloud-python/packages/google-cloud-media-translation/google/cloud/mediatranslation_v1beta1/types/media_translation.py": [
    {
      "type": "function",
      "name": "StreamingTranslateSpeechRequest",
      "description": "The top-level message sent by the client for the\n``StreamingTranslateSpeech`` method. Multiple\n``StreamingTranslateSpeechRequest`` messages are sent. The first\nmessage must contain a ``streaming_config`` message and must not\ncontain ``audio_content`` data. All subsequent messages must contain\n``audio_content`` data and must not contain a ``streaming_config``\nmessage.\n\nThis message has `oneof`_ fields (mutually exclusive fields).\nFor each oneof, at most one member field can be set at the same time.\nSetting any member of the oneof automatically clears all other\nmembers.\n\n.. _oneof: https://proto-plus-python.readthedocs.io/en/stable/fields.html#oneofs-mutually-exclusive-fields\n\nAttributes:\n    streaming_config (google.cloud.mediatranslation_v1beta1.types.StreamingTranslateSpeechConfig):\n        Provides information to the recognizer that specifies how to\n        process the request. The first\n        ``StreamingTranslateSpeechRequest`` message must contain a\n        ``streaming_config`` message.\n\n        This fi",
      "parameters": {
        "type": "object",
        "properties": {
          "streaming_config": {
            "description": "Provides information to the recognizer that specifies how to process the request. The first ``StreamingTranslateSpeechRequest`` message must contain a ``streaming_config`` message.  This field is a member of `oneof`_ ``streaming_request``.",
            "reference": "google.cloud.mediatranslation_v1beta1.types.StreamingTranslateSpeechConfig",
            "resolved_schema": {
              "type": "object",
              "properties": {
                "audio_config": {
                  "description": "Required. The common config for all the following audio contents.",
                  "type": "object",
                  "reference": "google.cloud.mediatranslation_v1beta1.types.TranslateSpeechConfig",
                  "resolved_schema": {
                    "type": "object",
                    "properties": {
                      "audio_encoding": {
                        "description": "Required. Encoding of audio data. Supported formats:  -  ``linear16``  Uncompressed 16-bit signed little-endian samples (Linear PCM).  -  ``flac``  ``flac`` (Free Lossless Audio Codec) is the recommended encoding because it is lossless--therefore recognition is not compromised--and requires only about half the bandwidth of ``linear16``.  -  ``mulaw``  8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.  -  ``amr``  Adaptive Multi-Rate Narrowband codec. ``sample_rate_hertz`` must be 8000.  -  ``amr-wb``  Adaptive Multi-Rate Wideband codec. ``sample_rate_hertz`` must be 16000.  -  ``ogg-opus``  Opus encoded audio frames in `Ogg <https://wikipedia.org/wiki/Ogg>`__ container. ``sample_rate_hertz`` must be one of 8000, 12000, 16000, 24000, or 48000.  -  ``mp3``  MP3 audio. Support all standard MP3 bitrates (which range from 32-320 kbps). When using this encoding, ``sample_rate_hertz`` has to match the sample rate of the file being used.",
                        "type": "string"
                      },
                      "source_language_code": {
                        "description": "Required. Source language code (BCP-47) of the input audio.",
                        "type": "string"
                      },
                      "target_language_code": {
                        "description": "Required. Target language code (BCP-47) of the output.",
                        "type": "string"
                      },
                      "sample_rate_hertz": {
                        "description": "Optional. Sample rate in Hertz of the audio data. Valid values are: 8000-48000. 16000 is optimal. For best results, set the sampling rate of the audio source to 16000 Hz. If that's not possible, use the native sample rate of the audio source (instead of re-sampling).",
                        "type": "integer"
                      },
                      "model": {
                        "description": "Optional. ``google-provided-model/video`` and ``google-provided-model/enhanced-phone-call`` are premium models. ``google-provided-model/phone-call`` is not premium model.",
                        "type": "string"
                      }
                    },
                    "required": [
                      "audio_encoding",
                      "source_language_code",
                      "target_language_code"
                    ]
                  }
                },
                "single_utterance": {
                  "description": "Optional. If ``false`` or omitted, the system performs continuous translation (continuing to wait for and process audio even if the user pauses speaking) until the client closes the input stream (gRPC API) or until the maximum time limit has been reached. May return multiple ``StreamingTranslateSpeechResult``\\ s with the ``is_final`` flag set to ``true``.  If ``true``, the speech translator will detect a single spoken utterance. When it detects that the user has paused or stopped speaking, it will return an ``END_OF_SINGLE_UTTERANCE`` event and cease translation. When the client receives 'END_OF_SINGLE_UTTERANCE' event, the client should stop sending the requests. However, clients should keep receiving remaining responses until the stream is terminated. To construct the complete sentence in a streaming way, one should override (if 'is_final' of previous response is false), or append (if 'is_final' of previous response is true).",
                  "type": "boolean"
                }
              },
              "required": [
                "audio_config"
              ]
            }
          },
          "audio_content": {
            "description": "The audio data to be translated. Sequential chunks of audio data are sent in sequential ``StreamingTranslateSpeechRequest`` messages. The first ``StreamingTranslateSpeechRequest`` message must not contain ``audio_content`` data and all subsequent ``StreamingTranslateSpeechRequest`` messages must contain ``audio_content`` data. The audio bytes must be encoded as specified in ``StreamingTranslateSpeechConfig``. Note: as with all bytes fields, protobuffers use a pure binary representation (not base64).  This field is a member of `oneof`_ ``streaming_request``.",
            "type": "object",
            "reference": "bytes"
          }
        }
      }
    }
  ]
}