{
  "\\Users\\AMD\\vidhra\\internal\\ast\\google-cloud-python\\packages\\google-cloud-videointelligence\\google\\cloud\\videointelligence_v1p2beta1\\types\\video_intelligence.py": [
    {
      "type": "request_class",
      "name": "AnnotateVideoRequest",
      "docstring": "Video annotation request.\n\nAttributes:\n    input_uri (str):\n        Input video location. Currently, only `Google Cloud\n        Storage <https://cloud.google.com/storage/>`__ URIs are\n        supported, which must be specified in the following format:\n        ``gs://bucket-id/object-id`` (other URI formats return\n        [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]).\n        For more information, see `Request\n        URIs <https://cloud.google.com/storage/docs/request-endpoints>`__.\n        A video URI may include wildcards in ``object-id``, and thus\n        identify multiple videos. Supported wildcards: '*' to match\n        0 or more characters; '?' to match 1 character. If unset,\n        the input video should be embedded in the request as\n        ``input_content``. If set, ``input_content`` should be\n        unset.\n    input_content (bytes):\n        The video data bytes. If unset, the input video(s) should be\n        specified via ``input_uri``. If set, ``input_uri`` should be\n     ",
      "attributes": {
        "input_uri": {
          "type": "str",
          "description": "Input video location. Currently, only `Google Cloud Storage <https://cloud.google.com/storage/>`__ URIs are supported, which must be specified in the following format: ``gs://bucket-id/object-id`` (other URI formats return [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see `Request URIs <https://cloud.google.com/storage/docs/request-endpoints>`__. A video URI may include wildcards in ``object-id``, and thus identify multiple videos. Supported wildcards: '*' to match 0 or more characters; '?' to match 1 character. If unset, the input video should be embedded in the request as ``input_content``. If set, ``input_content`` should be unset."
        },
        "input_content": {
          "type": "bytes",
          "description": "The video data bytes. If unset, the input video(s) should be specified via ``input_uri``. If set, ``input_uri`` should be unset."
        },
        "features": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.Feature]",
          "description": "Required. Requested video annotation features."
        },
        "video_context": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.VideoContext",
          "description": "Additional video context and/or feature-specific parameters."
        },
        "output_uri": {
          "type": "str",
          "description": "Optional. Location where the output (in JSON format) should be stored. Currently, only `Google Cloud Storage <https://cloud.google.com/storage/>`__ URIs are supported, which must be specified in the following format: ``gs://bucket-id/object-id`` (other URI formats return [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see `Request URIs <https://cloud.google.com/storage/docs/request-endpoints>`__."
        },
        "location_id": {
          "type": "str",
          "description": "Optional. Cloud region where annotation should take place. Supported cloud regions: ``us-east1``, ``us-west1``, ``europe-west1``, ``asia-east1``. If no region is specified, a region will be determined based on video file location."
        }
      }
    },
    {
      "type": "request_class",
      "name": "VideoContext",
      "docstring": "Video context and/or feature-specific parameters.\n\nAttributes:\n    segments (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.VideoSegment]):\n        Video segments to annotate. The segments may\n        overlap and are not required to be contiguous or\n        span the whole video. If unspecified, each video\n        is treated as a single segment.\n    label_detection_config (google.cloud.videointelligence_v1p2beta1.types.LabelDetectionConfig):\n        Config for LABEL_DETECTION.\n    shot_change_detection_config (google.cloud.videointelligence_v1p2beta1.types.ShotChangeDetectionConfig):\n        Config for SHOT_CHANGE_DETECTION.\n    explicit_content_detection_config (google.cloud.videointelligence_v1p2beta1.types.ExplicitContentDetectionConfig):\n        Config for EXPLICIT_CONTENT_DETECTION.\n    text_detection_config (google.cloud.videointelligence_v1p2beta1.types.TextDetectionConfig):\n        Config for TEXT_DETECTION.",
      "attributes": {
        "segments": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.VideoSegment]",
          "description": "Video segments to annotate. The segments may overlap and are not required to be contiguous or span the whole video. If unspecified, each video is treated as a single segment."
        },
        "label_detection_config": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.LabelDetectionConfig",
          "description": "Config for LABEL_DETECTION."
        },
        "shot_change_detection_config": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.ShotChangeDetectionConfig",
          "description": "Config for SHOT_CHANGE_DETECTION."
        },
        "explicit_content_detection_config": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.ExplicitContentDetectionConfig",
          "description": "Config for EXPLICIT_CONTENT_DETECTION."
        },
        "text_detection_config": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.TextDetectionConfig",
          "description": "Config for TEXT_DETECTION."
        }
      }
    },
    {
      "type": "request_class",
      "name": "LabelDetectionConfig",
      "docstring": "Config for LABEL_DETECTION.\n\nAttributes:\n    label_detection_mode (google.cloud.videointelligence_v1p2beta1.types.LabelDetectionMode):\n        What labels should be detected with LABEL_DETECTION, in\n        addition to video-level labels or segment-level labels. If\n        unspecified, defaults to ``SHOT_MODE``.\n    stationary_camera (bool):\n        Whether the video has been shot from a stationary (i.e.\n        non-moving) camera. When set to true, might improve\n        detection accuracy for moving objects. Should be used with\n        ``SHOT_AND_FRAME_MODE`` enabled.\n    model (str):\n        Model to use for label detection.\n        Supported values: \"builtin/stable\" (the default\n        if unset) and \"builtin/latest\".",
      "attributes": {
        "label_detection_mode": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.LabelDetectionMode",
          "description": "What labels should be detected with LABEL_DETECTION, in addition to video-level labels or segment-level labels. If unspecified, defaults to ``SHOT_MODE``."
        },
        "stationary_camera": {
          "type": "bool",
          "description": "Whether the video has been shot from a stationary (i.e. non-moving) camera. When set to true, might improve detection accuracy for moving objects. Should be used with ``SHOT_AND_FRAME_MODE`` enabled."
        },
        "model": {
          "type": "str",
          "description": "Model to use for label detection. Supported values: \"builtin/stable\" (the default if unset) and \"builtin/latest\"."
        }
      }
    },
    {
      "type": "request_class",
      "name": "ShotChangeDetectionConfig",
      "docstring": "Config for SHOT_CHANGE_DETECTION.\n\nAttributes:\n    model (str):\n        Model to use for shot change detection.\n        Supported values: \"builtin/stable\" (the default\n        if unset) and \"builtin/latest\".",
      "attributes": {
        "model": {
          "type": "str",
          "description": "Model to use for shot change detection. Supported values: \"builtin/stable\" (the default if unset) and \"builtin/latest\"."
        }
      }
    },
    {
      "type": "request_class",
      "name": "ExplicitContentDetectionConfig",
      "docstring": "Config for EXPLICIT_CONTENT_DETECTION.\n\nAttributes:\n    model (str):\n        Model to use for explicit content detection.\n        Supported values: \"builtin/stable\" (the default\n        if unset) and \"builtin/latest\".",
      "attributes": {
        "model": {
          "type": "str",
          "description": "Model to use for explicit content detection. Supported values: \"builtin/stable\" (the default if unset) and \"builtin/latest\"."
        }
      }
    },
    {
      "type": "request_class",
      "name": "TextDetectionConfig",
      "docstring": "Config for TEXT_DETECTION.\n\nAttributes:\n    language_hints (MutableSequence[str]):\n        Language hint can be specified if the\n        language to be detected is known a priori. It\n        can increase the accuracy of the detection.\n        Language hint must be language code in BCP-47\n        format.\n\n        Automatic language detection is performed if no\n        hint is provided.",
      "attributes": {
        "language_hints": {
          "type": "MutableSequence[str]",
          "description": "Language hint can be specified if the language to be detected is known a priori. It can increase the accuracy of the detection. Language hint must be language code in BCP-47 format.  Automatic language detection is performed if no hint is provided."
        }
      }
    },
    {
      "type": "request_class",
      "name": "VideoSegment",
      "docstring": "Video segment.\n\nAttributes:\n    start_time_offset (google.protobuf.duration_pb2.Duration):\n        Time-offset, relative to the beginning of the\n        video, corresponding to the start of the segment\n        (inclusive).\n    end_time_offset (google.protobuf.duration_pb2.Duration):\n        Time-offset, relative to the beginning of the\n        video, corresponding to the end of the segment\n        (inclusive).",
      "attributes": {
        "start_time_offset": {
          "type": "google.protobuf.duration_pb2.Duration",
          "description": "Time-offset, relative to the beginning of the video, corresponding to the start of the segment (inclusive)."
        },
        "end_time_offset": {
          "type": "google.protobuf.duration_pb2.Duration",
          "description": "Time-offset, relative to the beginning of the video, corresponding to the end of the segment (inclusive)."
        }
      }
    },
    {
      "type": "request_class",
      "name": "LabelSegment",
      "docstring": "Video segment level annotation results for label detection.\n\nAttributes:\n    segment (google.cloud.videointelligence_v1p2beta1.types.VideoSegment):\n        Video segment where a label was detected.\n    confidence (float):\n        Confidence that the label is accurate. Range: [0, 1].",
      "attributes": {
        "segment": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.VideoSegment",
          "description": "Video segment where a label was detected."
        },
        "confidence": {
          "type": "float",
          "description": "Confidence that the label is accurate. Range: [0, 1]."
        }
      }
    },
    {
      "type": "request_class",
      "name": "LabelFrame",
      "docstring": "Video frame level annotation results for label detection.\n\nAttributes:\n    time_offset (google.protobuf.duration_pb2.Duration):\n        Time-offset, relative to the beginning of the\n        video, corresponding to the video frame for this\n        location.\n    confidence (float):\n        Confidence that the label is accurate. Range: [0, 1].",
      "attributes": {
        "time_offset": {
          "type": "google.protobuf.duration_pb2.Duration",
          "description": "Time-offset, relative to the beginning of the video, corresponding to the video frame for this location."
        },
        "confidence": {
          "type": "float",
          "description": "Confidence that the label is accurate. Range: [0, 1]."
        }
      }
    },
    {
      "type": "request_class",
      "name": "Entity",
      "docstring": "Detected entity from video analysis.\n\nAttributes:\n    entity_id (str):\n        Opaque entity ID. Some IDs may be available in `Google\n        Knowledge Graph Search\n        API <https://developers.google.com/knowledge-graph/>`__.\n    description (str):\n        Textual description, e.g. ``Fixed-gear bicycle``.\n    language_code (str):\n        Language code for ``description`` in BCP-47 format.",
      "attributes": {
        "entity_id": {
          "type": "str",
          "description": "Opaque entity ID. Some IDs may be available in `Google Knowledge Graph Search API <https://developers.google.com/knowledge-graph/>`__."
        },
        "description": {
          "type": "str",
          "description": "Textual description, e.g. ``Fixed-gear bicycle``."
        },
        "language_code": {
          "type": "str",
          "description": "Language code for ``description`` in BCP-47 format."
        }
      }
    },
    {
      "type": "request_class",
      "name": "LabelAnnotation",
      "docstring": "Label annotation.\n\nAttributes:\n    entity (google.cloud.videointelligence_v1p2beta1.types.Entity):\n        Detected entity.\n    category_entities (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.Entity]):\n        Common categories for the detected entity. E.g. when the\n        label is ``Terrier`` the category is likely ``dog``. And in\n        some cases there might be more than one categories e.g.\n        ``Terrier`` could also be a ``pet``.\n    segments (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.LabelSegment]):\n        All video segments where a label was\n        detected.\n    frames (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.LabelFrame]):\n        All video frames where a label was detected.",
      "attributes": {
        "entity": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.Entity",
          "description": "Detected entity."
        },
        "category_entities": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.Entity]",
          "description": "Common categories for the detected entity. E.g. when the label is ``Terrier`` the category is likely ``dog``. And in some cases there might be more than one categories e.g. ``Terrier`` could also be a ``pet``."
        },
        "segments": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.LabelSegment]",
          "description": "All video segments where a label was detected."
        },
        "frames": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.LabelFrame]",
          "description": "All video frames where a label was detected."
        }
      }
    },
    {
      "type": "request_class",
      "name": "ExplicitContentFrame",
      "docstring": "Video frame level annotation results for explicit content.\n\nAttributes:\n    time_offset (google.protobuf.duration_pb2.Duration):\n        Time-offset, relative to the beginning of the\n        video, corresponding to the video frame for this\n        location.\n    pornography_likelihood (google.cloud.videointelligence_v1p2beta1.types.Likelihood):\n        Likelihood of the pornography content..",
      "attributes": {
        "time_offset": {
          "type": "google.protobuf.duration_pb2.Duration",
          "description": "Time-offset, relative to the beginning of the video, corresponding to the video frame for this location."
        },
        "pornography_likelihood": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.Likelihood",
          "description": "Likelihood of the pornography content.."
        }
      }
    },
    {
      "type": "request_class",
      "name": "ExplicitContentAnnotation",
      "docstring": "Explicit content annotation (based on per-frame visual\nsignals only). If no explicit content has been detected in a\nframe, no annotations are present for that frame.\n\nAttributes:\n    frames (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.ExplicitContentFrame]):\n        All video frames where explicit content was\n        detected.",
      "attributes": {
        "frames": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.ExplicitContentFrame]",
          "description": "All video frames where explicit content was detected."
        }
      }
    },
    {
      "type": "request_class",
      "name": "NormalizedBoundingBox",
      "docstring": "Normalized bounding box. The normalized vertex coordinates are\nrelative to the original image. Range: [0, 1].\n\nAttributes:\n    left (float):\n        Left X coordinate.\n    top (float):\n        Top Y coordinate.\n    right (float):\n        Right X coordinate.\n    bottom (float):\n        Bottom Y coordinate.",
      "attributes": {
        "left": {
          "type": "float",
          "description": "Left X coordinate."
        },
        "top": {
          "type": "float",
          "description": "Top Y coordinate."
        },
        "right": {
          "type": "float",
          "description": "Right X coordinate."
        },
        "bottom": {
          "type": "float",
          "description": "Bottom Y coordinate."
        }
      }
    },
    {
      "type": "request_class",
      "name": "VideoAnnotationResults",
      "docstring": "Annotation results for a single video.\n\nAttributes:\n    input_uri (str):\n        Video file location in `Google Cloud\n        Storage <https://cloud.google.com/storage/>`__.\n    segment_label_annotations (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.LabelAnnotation]):\n        Label annotations on video level or user\n        specified segment level. There is exactly one\n        element for each unique label.\n    shot_label_annotations (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.LabelAnnotation]):\n        Label annotations on shot level.\n        There is exactly one element for each unique\n        label.\n    frame_label_annotations (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.LabelAnnotation]):\n        Label annotations on frame level.\n        There is exactly one element for each unique\n        label.\n    shot_annotations (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.VideoSegment]):\n        Shot annotations. Each shot is represented as\n",
      "attributes": {
        "input_uri": {
          "type": "str",
          "description": "Video file location in `Google Cloud Storage <https://cloud.google.com/storage/>`__."
        },
        "segment_label_annotations": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.LabelAnnotation]",
          "description": "Label annotations on video level or user specified segment level. There is exactly one element for each unique label."
        },
        "shot_label_annotations": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.LabelAnnotation]",
          "description": "Label annotations on shot level. There is exactly one element for each unique label."
        },
        "frame_label_annotations": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.LabelAnnotation]",
          "description": "Label annotations on frame level. There is exactly one element for each unique label."
        },
        "shot_annotations": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.VideoSegment]",
          "description": "Shot annotations. Each shot is represented as a video segment."
        },
        "explicit_annotation": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.ExplicitContentAnnotation",
          "description": "Explicit content annotation."
        },
        "text_annotations": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.TextAnnotation]",
          "description": "OCR text detection and tracking. Annotations for list of detected text snippets. Each will have list of frame information associated with it."
        },
        "object_annotations": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.ObjectTrackingAnnotation]",
          "description": "Annotations for list of objects detected and tracked in video."
        },
        "error": {
          "type": "google.rpc.status_pb2.Status",
          "description": "If set, indicates an error. Note that for a single ``AnnotateVideoRequest`` some videos may succeed and some may fail."
        }
      }
    },
    {
      "type": "request_class",
      "name": "AnnotateVideoResponse",
      "docstring": "Video annotation response. Included in the ``response`` field of the\n``Operation`` returned by the ``GetOperation`` call of the\n``google::longrunning::Operations`` service.\n\nAttributes:\n    annotation_results (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.VideoAnnotationResults]):\n        Annotation results for all videos specified in\n        ``AnnotateVideoRequest``.",
      "attributes": {
        "annotation_results": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.VideoAnnotationResults]",
          "description": "Annotation results for all videos specified in ``AnnotateVideoRequest``."
        }
      }
    },
    {
      "type": "request_class",
      "name": "VideoAnnotationProgress",
      "docstring": "Annotation progress for a single video.\n\nAttributes:\n    input_uri (str):\n        Video file location in `Google Cloud\n        Storage <https://cloud.google.com/storage/>`__.\n    progress_percent (int):\n        Approximate percentage processed thus far.\n        Guaranteed to be 100 when fully processed.\n    start_time (google.protobuf.timestamp_pb2.Timestamp):\n        Time when the request was received.\n    update_time (google.protobuf.timestamp_pb2.Timestamp):\n        Time of the most recent update.",
      "attributes": {
        "input_uri": {
          "type": "str",
          "description": "Video file location in `Google Cloud Storage <https://cloud.google.com/storage/>`__."
        },
        "progress_percent": {
          "type": "int",
          "description": "Approximate percentage processed thus far. Guaranteed to be 100 when fully processed."
        },
        "start_time": {
          "type": "google.protobuf.timestamp_pb2.Timestamp",
          "description": "Time when the request was received."
        },
        "update_time": {
          "type": "google.protobuf.timestamp_pb2.Timestamp",
          "description": "Time of the most recent update."
        }
      }
    },
    {
      "type": "request_class",
      "name": "AnnotateVideoProgress",
      "docstring": "Video annotation progress. Included in the ``metadata`` field of the\n``Operation`` returned by the ``GetOperation`` call of the\n``google::longrunning::Operations`` service.\n\nAttributes:\n    annotation_progress (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.VideoAnnotationProgress]):\n        Progress metadata for all videos specified in\n        ``AnnotateVideoRequest``.",
      "attributes": {
        "annotation_progress": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.VideoAnnotationProgress]",
          "description": "Progress metadata for all videos specified in ``AnnotateVideoRequest``."
        }
      }
    },
    {
      "type": "request_class",
      "name": "NormalizedVertex",
      "docstring": "A vertex represents a 2D point in the image.\nNOTE: the normalized vertex coordinates are relative to the\noriginal image and range from 0 to 1.\n\nAttributes:\n    x (float):\n        X coordinate.\n    y (float):\n        Y coordinate.",
      "attributes": {
        "x": {
          "type": "float",
          "description": "X coordinate."
        },
        "y": {
          "type": "float",
          "description": "Y coordinate."
        }
      }
    },
    {
      "type": "request_class",
      "name": "NormalizedBoundingPoly",
      "docstring": "Normalized bounding polygon for text (that might not be aligned with\naxis). Contains list of the corner points in clockwise order\nstarting from top-left corner. For example, for a rectangular\nbounding box: When the text is horizontal it might look like: 0----1\n\\| \\| 3----2\n\nWhen it's clockwise rotated 180 degrees around the top-left corner\nit becomes: 2----3 \\| \\| 1----0\n\nand the vertex order will still be (0, 1, 2, 3). Note that values\ncan be less than 0, or greater than 1 due to trignometric\ncalculations for location of the box.\n\nAttributes:\n    vertices (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.NormalizedVertex]):\n        Normalized vertices of the bounding polygon.",
      "attributes": {
        "vertices": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.NormalizedVertex]",
          "description": "Normalized vertices of the bounding polygon."
        }
      }
    },
    {
      "type": "request_class",
      "name": "TextSegment",
      "docstring": "Video segment level annotation results for text detection.\n\nAttributes:\n    segment (google.cloud.videointelligence_v1p2beta1.types.VideoSegment):\n        Video segment where a text snippet was\n        detected.\n    confidence (float):\n        Confidence for the track of detected text. It\n        is calculated as the highest over all frames\n        where OCR detected text appears.\n    frames (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.TextFrame]):\n        Information related to the frames where OCR\n        detected text appears.",
      "attributes": {
        "segment": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.VideoSegment",
          "description": "Video segment where a text snippet was detected."
        },
        "confidence": {
          "type": "float",
          "description": "Confidence for the track of detected text. It is calculated as the highest over all frames where OCR detected text appears."
        },
        "frames": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.TextFrame]",
          "description": "Information related to the frames where OCR detected text appears."
        }
      }
    },
    {
      "type": "request_class",
      "name": "TextFrame",
      "docstring": "Video frame level annotation results for text annotation\n(OCR). Contains information regarding timestamp and bounding box\nlocations for the frames containing detected OCR text snippets.\n\nAttributes:\n    rotated_bounding_box (google.cloud.videointelligence_v1p2beta1.types.NormalizedBoundingPoly):\n        Bounding polygon of the detected text for\n        this frame.\n    time_offset (google.protobuf.duration_pb2.Duration):\n        Timestamp of this frame.",
      "attributes": {
        "rotated_bounding_box": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.NormalizedBoundingPoly",
          "description": "Bounding polygon of the detected text for this frame."
        },
        "time_offset": {
          "type": "google.protobuf.duration_pb2.Duration",
          "description": "Timestamp of this frame."
        }
      }
    },
    {
      "type": "request_class",
      "name": "TextAnnotation",
      "docstring": "Annotations related to one detected OCR text snippet. This\nwill contain the corresponding text, confidence value, and frame\nlevel information for each detection.\n\nAttributes:\n    text (str):\n        The detected text.\n    segments (MutableSequence[google.cloud.videointelligence_v1p2beta1.types.TextSegment]):\n        All video segments where OCR detected text\n        appears.",
      "attributes": {
        "text": {
          "type": "str",
          "description": "The detected text."
        },
        "segments": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.TextSegment]",
          "description": "All video segments where OCR detected text appears."
        }
      }
    },
    {
      "type": "request_class",
      "name": "ObjectTrackingFrame",
      "docstring": "Video frame level annotations for object detection and\ntracking. This field stores per frame location, time offset, and\nconfidence.\n\nAttributes:\n    normalized_bounding_box (google.cloud.videointelligence_v1p2beta1.types.NormalizedBoundingBox):\n        The normalized bounding box location of this\n        object track for the frame.\n    time_offset (google.protobuf.duration_pb2.Duration):\n        The timestamp of the frame in microseconds.",
      "attributes": {
        "normalized_bounding_box": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.NormalizedBoundingBox",
          "description": "The normalized bounding box location of this object track for the frame."
        },
        "time_offset": {
          "type": "google.protobuf.duration_pb2.Duration",
          "description": "The timestamp of the frame in microseconds."
        }
      }
    },
    {
      "type": "request_class",
      "name": "ObjectTrackingAnnotation",
      "docstring": "Annotations corresponding to one tracked object.\n\nThis message has `oneof`_ fields (mutually exclusive fields).\nFor each oneof, at most one member field can be set at the same time.\nSetting any member of the oneof automatically clears all other\nmembers.\n\n.. _oneof: https://proto-plus-python.readthedocs.io/en/stable/fields.html#oneofs-mutually-exclusive-fields\n\nAttributes:\n    segment (google.cloud.videointelligence_v1p2beta1.types.VideoSegment):\n        Non-streaming batch mode ONLY.\n        Each object track corresponds to one video\n        segment where it appears.\n\n        This field is a member of `oneof`_ ``track_info``.\n    track_id (int):\n        Streaming mode ONLY. In streaming mode, we do not know the\n        end time of a tracked object before it is completed. Hence,\n        there is no VideoSegment info returned. Instead, we provide\n        a unique identifiable integer track_id so that the customers\n        can correlate the results of the ongoing\n        ObjectTrackAnnotation of the same track_i",
      "attributes": {
        "segment": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.VideoSegment",
          "description": "Non-streaming batch mode ONLY. Each object track corresponds to one video segment where it appears.  This field is a member of `oneof`_ ``track_info``."
        },
        "track_id": {
          "type": "int",
          "description": "Streaming mode ONLY. In streaming mode, we do not know the end time of a tracked object before it is completed. Hence, there is no VideoSegment info returned. Instead, we provide a unique identifiable integer track_id so that the customers can correlate the results of the ongoing ObjectTrackAnnotation of the same track_id over time.  This field is a member of `oneof`_ ``track_info``."
        },
        "entity": {
          "type": "google.cloud.videointelligence_v1p2beta1.types.Entity",
          "description": "Entity to specify the object category that this track is labeled as."
        },
        "confidence": {
          "type": "float",
          "description": "Object category's labeling confidence of this track."
        },
        "frames": {
          "type": "MutableSequence[google.cloud.videointelligence_v1p2beta1.types.ObjectTrackingFrame]",
          "description": "Information corresponding to all frames where this object track appears."
        }
      }
    }
  ]
}